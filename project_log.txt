My Sahayak Project Log
Project Overview
My Goal: To build "Sahayak," an AI assistant that can read hardware manuals (RAG) and look at photos (VLM) to help a technician with repairs.
________________________________________

Phase 1: Building the "Memory" (The RAG Backend)

Status: âœ… Complete

Goal
The goal of this phase was to read an Arduino PDF manual and turn it into a smart, searchable database (a "vector store"). This database is the "Memory" that the AI will use to answer questions.

Key Concepts
â€¢	RAG (Retrieval-Augmented Generation): This is the main concept. We "Retrieve" (find) the right info from our PDF before letting the AI "Generate" an answer. This makes the AI an expert on our document.
â€¢	Vector Store: This is the "memory" itself. It's a special database (in our case, the vectorstore_faiss folder) that stores the meaning of the text as lists of numbers (called "vectors").
â€¢	Embeddings: This is the "translator" model (instructor-large) that reads a text chunk and turns its meaning into a "vector."

Code & Libraries
â€¢	setup_rag.py: The script I wrote to build the memory.
â€¢	langchain / langchain_community: The main "toolkit" for building RAG pipelines.
â€¢	PyMuPDFLoader: The tool used to load and read the text from the PDF file.
â€¢	RecursiveCharacterTextSplitter: The tool used to "chop" the PDF's text into small, overlapping pieces.
â€¢	HuggingFaceInstructEmbeddings: The "translator" model (1.5GB) that understands the meaning of the text chunks.
â€¢	FAISS: The "filing cabinet" or database. It stores the "vectors" and lets us search them very fast.
â€¢	InstructorEmbedding: A helper library that the HuggingFaceInstructEmbeddings model needs to run.

Process (What I Did)
1.	Created a docs folder and put the pdf files inside it.
2.	Installed all the Python libraries (like langchain, faiss-cpu, pymupdf, etc.).
3.	Wrote the setup_rag.py script.
4.	Ran python setup_rag.py in the terminal.
5.	This script loaded the PDF, split it into 54 chunks, translated them into vectors, and saved them all into a new vectorstore_faiss folder.
6.	The script finished by running a test_rag_query() and successfully printed the 3 most relevant chunks from the PDF, proving the "memory" works.

Problems & Fixes
â€¢	Problem: ModuleNotFoundError: No module named 'langchain.text_splitter'.
o	Fix: langchain was updated. I had to run pip install langchain-text-splitters and change the import line in my code.
â€¢	Problem: ImportError: cannot import name 'HugGingFaceInstructEmbeddings'.
o	Fix: This was a typo in the code. I corrected HugGingFace to HuggingFace in setup_rag.py.
â€¢	Problem: ImportError: Dependencies for InstructorEmbedding not found.
o	Fix: The embedding model needed a helper library. I ran pip install InstructorEmbedding to fix it.
________________________________________


Phase 2: Building the "Brain" & "Face" (The Chat App)
Status: ðŸ”² In Progress
Goal
(You can fill this in) To create a web app with a chat window. When I type a question, the app will use the "memory" (from Phase 1) and an AI "Brain" (Gemini) to give me a smart answer.
Key Concepts
â€¢	(Fill in as we go)
â€¢	Streamlit:
â€¢	API (Application Programming Interface):
â€¢	LLM (Large Language Model):
Code & Libraries
â€¢	(Fill in as we go)
â€¢	app.py:
â€¢	streamlit:
â€¢	google-generativeai:
â€¢	.env:
Process (What I Did)
(Log your steps here as you do them. e.g., "1. Ran pip install streamlit...")
1.
2.
3.
Problems & Fixes
(Log any errors you hit here)
â€¢	Problem:
o	Fix:
________________________________________
Phase 3: Building the "Eyes" (The VLM)
Status: ðŸ”² Not Started
Goal
(Fill in when we get here) To add a "Take Photo" button. The AI should "see" the photo, identify the hardware or problem, and use that visual info to give an even better answer.
________________________________________
Does this template work for you? We can start Phase 2, Step 1 now.

